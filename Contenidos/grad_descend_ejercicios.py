# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lRkbtRbnxR-sQS0qQuFLl6nlnKL2cN7g
"""

# Regresión lineal con Gradiente Dscendiente

import numpy as np  # cuando trabajamos con vectores
import matplotlib.pyplot as plt
import sympy as sym  # nos permite aplicar deribadas parciales
from sklearn.linear_model import LinearRegression

# variable independiente
experiencia = np.array([[5], [6], [13]])

# variable objetivo
salario = np.array([6.85, 16.83, 26.84])

# Modelo lineal creado con los mínimos cuadrados
modelo = LinearRegression().fit(experiencia.reshape(3, -1), salario)

# visualización de datos y modelo
print("Intersección con eje Y (b): %0.2f"  % modelo.intercept_)
print("Pendiente (m): %0.2f" % modelo.coef_[0])
print("Suma de cuadrados de los Residuos (RSS): %0.2f" % ((salario - modelo.predict(experiencia))**2).sum())

plt.figure(figsize=(8, 6)) # tamaño de la figura

plt.scatter(experiencia, salario, color="gold", s=250, marker="o", label="Valor verdadero")

plt.scatter(experiencia, modelo.predict(experiencia), color="blue", s=250, marker="P", label="Valor predicho")

plt.plot(experiencia, modelo.predict(experiencia), color="deeppink", label="Modelo lineal")

experiencia = experiencia.reshape(3)

plt.ylabel("Salario en miles de Pesos ($)", size = 16)
plt.xlabel("Años de experiencia", size = 16)
plt.legend(bbox_to_anchor = (1.3, 0.5))

plt.grid()  # Permite colocar lineas cuadriculares en la grafica
plt.box(False) #False elimina la linea del borde de la grafica
plt.show()

# ======================================================================================================================

# Diferente Modelos con diferentes pendientes

# Creación de multiples pendientes para exploración
pendientes = np.arange(2.5, 1.6, -0.1)

# Vector para almacenar el error de los diferentes modelos
errores = np.array([])

# Visualización de modelos
plt.figure(figsize=(8, 6))

for pendiente in pendientes:
  # Error del modelo (suma de cuadrados de los Residuos)
  error = ((pendiente * experiencia - salario)**2).sum()

  # Visualización de un modelo para un pendiente dada
  plt.plot(experiencia, pendiente * experiencia, linewidth = 4,
           label = "m: %0.2f | error: %0.2f" % (pendiente, error))
  errores = np.append(errores, error)

plt.scatter(experiencia, salario, color="gold", s=250, marker="o", label="Valor verdadero")

plt.ylabel("Salario en miles de Pesos ($)", size = 16)
plt.xlabel("Años de experiencia", size = 16)
plt.legend(bbox_to_anchor = (1, 0.5))

plt.grid()  # Permite colocar lineas cuadriculares en la grafica
plt.box(False) #False elimina la linea del borde de la grafica
plt.show()

# ======================================================================================================================
# Visualización de los Errores (RSS)

plt.figure(figsize=(7.5, 6))
plt.title("Suma de cuadrados de los Residuos (RSS)", size = 16)
plt.ylabel("Error (RSS)", size = 16)
plt.xlabel("Pendiente (m)", size = 16)
plt.scatter(pendientes, errores, color = "purple", marker = "D", s = 99) # Dispersión
plt.grid()
plt.box(False)
plt.show()

# ======================================================================================================================
# EJERCICIO 1

# Función de Error a ser minimizada
# Suma de cuadrados de los residuos (RSS)

# RSS(y, ^y) = (Yi — ji)^2
# Yi = salario
# ^Yi = m * experienciai
# Donde y son los valores verdaderos, ^y son los valores predichos y n es el tamaño de la muestra.

import sympy as sym
import numpy as np
# persona 1: (6.85, 5) persona 2: (16.83, 6) persona 3: (21.84, 13)


# Creación de multiples pendientes para exploración
# pendientes = np.arange(2.5, 1.6, -0.1)

m = sym.Symbol("m")

# Función de error
error = (6.85 - m*5)**2 + (16.83 - m*6)**2 + (21.84 - m*13)**2

# Derivada de la función de error
derivada = sym.diff(error, m)

# print(derivada)

for pendiente in pendientes:
  print(derivada, "Evaluación  %0.2f" % derivada.evalf(subs={m : pendiente}))

# ======================================================================================================================
# Visualización  - Derivada de la función de error

plt.figure(figsize=(7.5, 6))

for i in range(0, len(errores), 1):
  # Error de un modelo dado
  plt.scatter(pendientes[i], errores[i], label = "%0.2f" % derivada.evalf(subs={m: pendientes[i]}), marker = "D", s=200)

  # Evaluación de la derivada para un error dado
  pendiente = derivada.evalf(subs={m : pendientes[i]})

  plt.plot(np.array([1.7, 2.6]), np.array([1.7, 2.6])*pendiente + pendiente*(-2.3 + i * 0.05) + errores.min())


# parra mostrar la grafica de la derivada
plt.title("Derivada de la función de error", size = 16)
plt.ylabel("Error (RSS)", size = 16)
plt.xlabel("Pendiente (m)", size = 16)
plt.ylim(30, 70)

plt.grid()
plt.box(False)
plt.show()

# ======================================================================================================================
# Generalizando para 2 o más parámetros: m y b

# Datos casi iguales, pero con un cambio en la Experiencia

# variable independiente
experiencia = np.array([[5], [8], [13]])

# variable objetivo
salario = np.array([6.85, 16.83, 26.84])

# Modelo lineal creando con el métodos de mínimos cuadrados
modelo = LinearRegression().fit(experiencia.reshape(3, -1), salario)

# Visualización de datos y modelo
print('Intersección con el eje Y (b): %0.2f' % modelo.intercept_)
print('Pendiente (m): %0.2f' % modelo.coef_[0])
print('La suma de los cuedrados de los residuos (RSS: %0.2f)' % ((salario - modelo.predict(experiencia))**2).sum())

plt.figure(figsize=(8, 6))

plt.scatter(experiencia, salario, color="gold", s=250, marker="o", label="Valor verdadero")

plt.scatter(experiencia, modelo.predict(experiencia), color="blue", s=250, marker="P", label="Valor predicho")

plt.plot(experiencia, modelo.predict(experiencia), linewidth = 4, color="deeppink", label="Modelo lineal")

plt.ylabel("Salario en miles de Pesos ($)", size = 16)
plt.xlabel("Años de experiencia", size = 16)
plt.legend(bbox_to_anchor = (1.3, 0.5))

plt.grid()
plt.box(False)
plt.show()

# ======================================================================================================================
# Función de Error (m, b)
def error_RSS(m, b):
  return (6.85 - (m*5 + b))**2 + (16.83 - (m*8 + b))**2 + (26.84 - (m*13 + b))

# Generación de rejilla (pendientes, interceptos, errores)

puntos = np.zeros(shape=(400, 3))


i = 0
for pendiente in np.arange(0, 5, 0.25):
  for intercepto in np.arange(-10, 0, 0.50):
    puntos[i][0] = pendiente
    puntos[i][1] = intercepto
    puntos[i][2] = error_RSS(pendiente, intercepto)
    i += 1

fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection = "3d")

# Mínimo global
ax.scatter(2.45, -4.38, marker = "o", c = "cyan", s = 800)

# Todos los otros puntos
ax.scatter(puntos.T[0], puntos.T[1], puntos.T[2], marker = 'o', c = 'purple', s = 200, alpha = 0.15)

ax.set_xlabel('Pendiente (m)', size = 14)
ax.set_ylabel('Interpreto (b)', size =  14)
ax.set_zlabel('Error', size = 14)

# ax.view_init(0, -30)
plt.show()

# ======================================================================================================================
# DERIVADAS PARCIALES DE LA FUNCIÓN DE ERROR

m, b = sym.symbols("m b")
error = (6.85 - (m*5 + b))**2 + (16.83 - (m*8 + b))**2 + (26.84 - (m*13 + b))

derivada_error_m = sym.diff(error, m)
derivada_error_b = sym.diff(error, b)

print(derivada_error_m)
print(derivada_error_b)

# ======================================================================================================================
# Pseudocódigo del Gradiente Descendiente

# 1. iteraciones máximas = 10000
# 2. iteración = O
# 3.tasa de aprendizaje = 0.001
# 4. inicialización aleatoria de parámetros
# 5. mientras (iteración < iteraciones máximas) hacer:
# 6.calcular gradientes en la posición actual
# 7.actualizar parámetros en función de los gradientes y la tasa de aprendizaje
# 8.iteración += 1

# EJERCICIO 2
# Implementación del Gradiente Descendiente

iteracion_maxima  = 10000
tasa_aprendisaje = 0.001

# Inicialización aleatoria del Pendiente y Intercepto
pendiente = 0
intercepto = -10

errores = np.zeros(shape=(10000, 3))

for i in range(iteracion_maxima):
  # Cálculo de los gradientes
  gradientes = [derivada_error_m.evalf(subs = {m : pendiente, b : intercepto}),
                derivada_error_b.evalf(subs = {m : pendiente, b : intercepto})]

  # Actualización de parámetros
  pendiente = pendiente - tasa_aprendisaje * gradientes[0]
  intercepto = intercepto - tasa_aprendisaje * gradientes[1]

  # Registro de los errores
  errores[i][0] = pendiente
  errores[i][1] = intercepto
  errores[i][2] = error_RSS(pendiente, intercepto)

# Valores calculados vía el método de los mínimos cuadrados de sklearn
# Pendiente (m): 2.44898163
# Intercepto (b) -4.382390816326527

print("Derivada pendiente (m) = ", gradientes[0])
print("Derivada intercepto (b) = ", gradientes[1])
print("m = ", pendiente, "b = ", intercepto)

# ======================================================================================================================
# Visualización del Descenso del Error
fig = plt.figure(figsize=(8,8))
ax = fig.add_subplot(projection = "3d")

# Camino decreciente de errores
ax.scatter(errores.T[0], errores.T[1], errores.T[2], marker = "o", c = "cyan", s = 800, alpha = 0.5)

# Todos los otros puntos
ax.scatter(puntos.T[0], puntos.T[1], puntos.T[2], marker = 'o', c = 'purple', s = 200, alpha = 0.15)

ax.set_xlabel('Pendiente (m)', size = 14)
ax.set_ylabel('Intercepto (b)', size = 14)
ax.set_zlabel('Error', size = 14)
plt.show()