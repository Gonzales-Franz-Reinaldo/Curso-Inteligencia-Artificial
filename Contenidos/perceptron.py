# -*- coding: utf-8 -*-
"""Perceptrón.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NDo-rDazRjE_zoqf_wt6U79sbrSK3OL_
"""

# Perceptrón La Base de las Redes Neuronales Artificiales de Machine Learning

# Datos linealmente separables
import numpy as np
import matplotlib.pyplot as plt

# Datos de 10 personas -> [edad, ahorro]
personas = np.array([
    [0.3, 0.4], [0.4, 0.3],
    [0.3, 0.2], [0.4, 0.1],
    [0.5, 0.2], [0.4, 0.8],
    [0.6, 0.8], [0.5, 0.6],
    [0.7, 0.6], [0.8, 0.5]
])

# 1 : aprobada   0 : denegado
clases = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

# Gráfica de dispersión (edad, ahorro)
plt.figure(figsize = (7, 7))
plt.title("¿Tarjeta Platinum?", fontsize = 20)
plt.scatter(personas[clases == 0].T[0],
            personas[clases == 0].T[1],
            marker = "x", s=180, color = "red",
            linewidths=5, label="Denegada")

plt.scatter(personas[clases == 1].T[0],
            personas[clases == 1].T[1],
            marker='o', s=180, color = 'blue',
            linewidths=5, label="Aprobada")

plt.xlabel("Edad", fontsize=15)
plt.ylabel("Ahorro", fontsize=15)
plt.legend(bbox_to_anchor=(1.3, 0.15))
plt.box(False)
plt.xlim(0, 1.01)
plt.ylim(0, 1.01)

plt.grid()
plt.show()

# ================================================================================================

# Función de Activación (Escalón)
# w1*x1 + w2*x2 + .... + wn*xn

def activacion(pesos, x, b):
  z = pesos * x

  if z.sum() + b > 0:
    return 1
  else:
    return 0

pesos = np.random.uniform(-1, 1, size = 2)
b = np.random.uniform(-1, 1)

pesos, b, activacion(pesos, [0.5, 0.5], b)

# ================================================================================================

# Pseudocódigo del Perceptrón
# Ejercicio 1

# 0. Inicializar los pesos y el umbral
# 1. epocas máximas = 100
# 2. epoca = 0
# 3. tasa de aprendizaje = 0.01
# 4. Mientras (epoca < epocas máximas) hacer:
# 5. Para cada instancia de entrenamiento hacer:
# 6. Calcula salida del perceptrón para esa estancia
# 7. Calcula el error
# 8. Actualiza pesos y umbral usando la tasa de aprendizaje, la instancia y el error
# 9. epoca += 1

# Entrenamiento del Perceptrón

pesos = np.random.uniform(-1, 1, size=2)
b = np.random.uniform(-1, 1)

tasa_aprendizaje = 0.01
epocas = 100

for epoca in range(epocas):
  error_total = 0
  for i in range(len(personas)):
    prediccion = activacion(pesos, personas[i], b)
    error = clases[i] - prediccion
    error_total += error**2
    pesos[0] += tasa_aprendizaje * personas[i][0] * error
    pesos[1] += tasa_aprendizaje * personas[i][1] * error
    b += tasa_aprendizaje * error
  print(error_total, end=" ")


activacion(pesos, [0.5, 0.8], b)

# ================================================================================================

# Zonas de aprobación
# Gráfica de dispersión [edad, ahorro]

plt.figure(figsize=(6, 5), dpi=200)
plt.title("¿Tarjeta Platinum?", fontsize=20)

plt.scatter(personas[clases == 0].T[0],
            personas[clases == 0].T[1],
            marker="x", s=180, color="red",
            linewidths=5, label="denegado")

plt.scatter(personas[clases == 1].T[0],
            personas[clases == 1].T[1],
            marker="o", s=180, color="blue",
            linewidths=5, label="Aprobado")

for edad in np.arange(0, 1, 0.05):
  for ahorro in np.arange(0, 1, 0.05):
    color = activacion(pesos, [edad, ahorro], b)

    if color == 1:
      plt.scatter(edad, ahorro, marker='s', s=110, color="blue", alpha=0.2, linewidths=0)
    else:
      plt.scatter(edad, ahorro, marker='s', s=110, color="red", alpha=0.2, linewidths=0)


plt.xlabel("Edad", fontsize=15)
plt.ylabel("Ahorro", fontsize=15)
plt.legend(bbox_to_anchor=(1.3, 0.15))
plt.box(False)
plt.xlim(0, 1.01)
plt.ylim(0, 1.01)
plt.show()

# ================================================================================================

# Perceptrón con Scikit-learn

from sklearn.linear_model import Perceptron

perceptron = Perceptron().fit(personas, clases)
perceptron.predict([[0.2, 0.2], [0.8, 0.8]])